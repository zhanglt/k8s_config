apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
    seccomp.security.alpha.kubernetes.io/pod: docker/default
  creationTimestamp: 2018-12-03T16:59:57Z
  generateName: kube-dns-548976df6c-
  labels:
    k8s-app: kube-dns
    pod-template-hash: "456"
  name: kube-dns-548976df6c-d9kkv
  #namespace: kube-system
  ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-548976df6c
      uid: b589a851-f71b-11e8-af4f-42010a800072
  resourceVersion: "50572715"
  selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-548976df6c-d9kkv
  uid: dd4bbbd4-f71c-11e8-af4f-42010a800072
spec:
  containers:
    - args:
        - --domain=cluster.local.
        - --dns-port=10053
        - --config-dir=/kube-dns-config
        - --v=2
      env:
        - name: PROMETHEUS_PORT
          value: "10055"
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.13
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/kubedns
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kubedns
      ports:
        - containerPort: 10053
          name: tcp-dns-local
          protocol: TCP
        - containerPort: 10055
          name: metrics
          protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /kube-dns-config
          name: kube-dns-config
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-dns-token-lwn8l
          readOnly: true
    - args:
        - -v=2
        - -logtostderr
        - -configDir=/etc/k8s/dns/dnsmasq-nanny
        - -restartDnsmasq=true
        - --
        - -k
        - --cache-size=1000
        - --no-negcache
        - --log-facility=-
        - --server=/cluster.local/127.0.0.1#10053
        - --server=/in-addr.arpa/127.0.0.1#10053
        - --server=/ip6.arpa/127.0.0.1#10053
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.13
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/dnsmasq
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dnsmasq
      ports:
        - containerPort: 53
          name: tcp-dns
          protocol: TCP
      resources:
        requests:
          cpu: 150m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /etc/k8s/dns/dnsmasq-nanny
          name: kube-dns-config
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-dns-token-lwn8l
          readOnly: true
    - args:
        - --v=2
        - --logtostderr
        - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
        - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.13
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: sidecar
      ports:
        - containerPort: 10054
          name: metrics
          protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-dns-token-lwn8l
          readOnly: true
    - command:
        - /monitor
        - --component=kubedns
        - --target-port=10054
        - --stackdriver-prefix=container.googleapis.com/internal/addons
        - --api-override=https://monitoring.googleapis.com/
        - --whitelisted-metrics=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
        - --pod-id=$(POD_NAME)
        - --namespace-id=$(POD_NAMESPACE)
        - --v=2
      env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
      image: gcr.io/google-containers/prometheus-to-sd:v0.2.3
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-dns-token-lwn8l
          readOnly: true
  dnsPolicy: Default
  nodeName: gke-istio-test-default-pool-866a0405-ftch
  priority: 2000000000
  priorityClassName: system-cluster-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: kube-dns
  serviceAccountName: kube-dns
  terminationGracePeriodSeconds: 30
  tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  volumes:
    - configMap:
        defaultMode: 420
        name: kube-dns
        optional: true
      name: kube-dns-config
    - name: kube-dns-token-lwn8l
      secret:
        defaultMode: 420
        secretName: kube-dns-token-lwn8l
status:
  conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-12-03T17:00:00Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-12-03T17:00:20Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: null
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: 2018-12-03T16:59:57Z
      status: "True"
      type: PodScheduled
  containerStatuses:
    - containerID: docker://676f6c98bfa136315c4ccf0fe40e7a56cbf9ac85128e94310eae82f191246b3e
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.13
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:45df3e8e0c551bd0c79cdba48ae6677f817971dcbd1eeed7fd1f9a35118410e4
      lastState: {}
      name: dnsmasq
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-12-03T17:00:14Z
    - containerID: docker://93fd0664e150982dad0481c5260183308a7035a2f938ec50509d586ed586a107
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.13
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:618a82fa66cf0c75e4753369a6999032372be7308866fc9afb381789b1e5ad52
      lastState: {}
      name: kubedns
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-12-03T17:00:10Z
    - containerID: docker://e823b79a0a48af75f2eebb1c89ba4c31e8c1ee67ee0d917ac7b4891b67d2cd0f
      image: gcr.io/google-containers/prometheus-to-sd:v0.2.3
      imageID: docker-pullable://gcr.io/google-containers/prometheus-to-sd@sha256:be220ec4a66275442f11d420033c106bb3502a3217a99c806eef3cf9858788a2
      lastState: {}
      name: prometheus-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-12-03T17:00:18Z
    - containerID: docker://74223c401a8dac04b8bd29cdfedcb216881791b4e84bb80a15714991dd18735e
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.13
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:cedc8fe2098dffc26d17f64061296b7aa54258a31513b6c52df271a98bb522b3
      lastState: {}
      name: sidecar
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-12-03T17:00:16Z
  hostIP: 10.128.0.5
  phase: Running
  podIP: 10.40.1.4
  qosClass: Burstable
  startTime: 2018-12-03T17:00:00Z
---
apiVersion: v1
kind: Node
metadata:
  annotations:
    container.googleapis.com/instance_id: "1656674321116487208"
    node.alpha.kubernetes.io/ttl: "0"
    volumes.kubernetes.io/controller-managed-attach-detach: "true"
  creationTimestamp: 2018-12-03T16:59:36Z
  labels:
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/fluentd-ds-ready: "true"
    beta.kubernetes.io/instance-type: n1-standard-4
    beta.kubernetes.io/os: linux
    cloud.google.com/gke-nodepool: default-pool
    cloud.google.com/gke-os-distribution: cos
    failure-domain.beta.kubernetes.io/region: us-central1
    failure-domain.beta.kubernetes.io/zone: us-central1-a
    kubernetes.io/hostname: gke-istio-test-default-pool-866a0405-ftch
  name: gke-istio-test-default-pool-866a0405-ftch
  resourceVersion: "64030615"
  selfLink: /api/v1/nodes/gke-istio-test-default-pool-866a0405-ftch
  uid: d0cad69e-f71c-11e8-af4f-42010a800072
spec:
  podCIDR: 10.40.1.0/24
  providerID: gce://nathanmittler-istio-test/us-central1-a/gke-istio-test-default-pool-866a0405-ftch
status:
  addresses:
    - address: 10.128.0.5
      type: InternalIP
    - address: 35.192.33.12
      type: ExternalIP
    - address: gke-istio-test-default-pool-866a0405-ftch
      type: Hostname
  allocatable:
    cpu: 3920m
    ephemeral-storage: "47093746742"
    hugepages-2Mi: "0"
    memory: 12699980Ki
    pods: "110"
  capacity:
    cpu: "4"
    ephemeral-storage: 98868448Ki
    hugepages-2Mi: "0"
    memory: 15399244Ki
    pods: "110"
  conditions:
    - lastHeartbeatTime: 2019-01-30T17:34:22Z
      lastTransitionTime: 2018-12-03T17:04:22Z
      message: node is functioning properly
      reason: UnregisterNetDevice
      status: "False"
      type: FrequentUnregisterNetDevice
    - lastHeartbeatTime: 2019-01-30T17:34:22Z
      lastTransitionTime: 2018-12-03T16:59:20Z
      message: kernel has no deadlock
      reason: KernelHasNoDeadlock
      status: "False"
      type: KernelDeadlock
    - lastHeartbeatTime: 2018-12-03T16:59:46Z
      lastTransitionTime: 2018-12-03T16:59:46Z
      message: RouteController created a route
      reason: RouteCreated
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: 2019-01-30T17:35:16Z
      lastTransitionTime: 2018-12-03T16:59:36Z
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: 2019-01-30T17:35:16Z
      lastTransitionTime: 2018-12-03T16:59:36Z
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: 2019-01-30T17:35:16Z
      lastTransitionTime: 2018-12-03T16:59:36Z
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: 2019-01-30T17:35:16Z
      lastTransitionTime: 2018-12-03T16:59:36Z
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: 2019-01-30T17:35:16Z
      lastTransitionTime: 2018-12-03T16:59:57Z
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
  daemonEndpoints:
    kubeletEndpoint:
      Port: 10250
  images:
    - names:
        - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:a33f69d0034fdce835a1eb7df8a051ea74323f3fc30d911bbd2e3f2aef09fc93
        - gcr.io/stackdriver-agents/stackdriver-logging-agent:0.3-1.5.34-1-k8s-1
      sizeBytes: 554981103
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:23a52850819d5196d66e8e20f4f63f314f779716f830e1d109ad0e24b1f0df43
      sizeBytes: 446407220
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:9949bc22667ef88e54ae91700a64bf1459e8c14ed92b870b7ec2f630e14cf3c1
      sizeBytes: 446407220
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:e338c2c5cbc379db24c5b2d67a4acc9cca9a069c2927217fca0ce7cbc582d312
      sizeBytes: 446398900
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:039dbddf8498eff82b25b04cd35c81c4f3e350a1c34e1b128bb0199d6e7d4f98
        - gcr.io/nathanmittler-istio-test/proxyv2:latest
      sizeBytes: 368758526
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:cdd2f527b4bd392b533d2d0e62c257c19d5a35a6b5fc3512aa327c560866aec1
      sizeBytes: 344257154
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:6ec1dced4cee8569c77817927938fa4341f939e0dddab511bc3ee8724d652ae2
      sizeBytes: 344257154
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:9d502fd29961bc3464f7906ac0e86b07edf01cf4892352ef780e55b3525fb0b8
      sizeBytes: 344257154
    - names:
        - gcr.io/nathanmittler-istio-test/proxyv2@sha256:3f4115cd8c26a17f6bf8ea49f1ff5b875382bda5a6d46281c70c886e802666b0
      sizeBytes: 344257154
    - names:
        - gcr.io/nathanmittler-istio-test/app@sha256:e141f14e7d872dcf855e09eceb411dc427188d0617fd18d14139db4ca99d2d0b
        - gcr.io/nathanmittler-istio-test/app:latest
      sizeBytes: 315430434
    - names:
        - gcr.io/nathanmittler-istio-test/app@sha256:430bdea6c4c1447dff4e8d2e9632da5211529375c45a22722dcc1500da9b95dd
      sizeBytes: 291266578
    - names:
        - gcr.io/nathanmittler-istio-test/app@sha256:ffaa484f413d324047fe000fbcc90d9195da99b681feb7c2ce8ce38f71724aec
      sizeBytes: 291230128
    - names:
        - k8s.gcr.io/node-problem-detector@sha256:f95cab985c26b2f46e9bd43283e0bfa88860c14e0fb0649266babe8b65e9eb2b
        - k8s.gcr.io/node-problem-detector:v0.4.1
      sizeBytes: 286572743
    - names:
        - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:441f6b6a118173ec3d17d5b0c1e74824fadad6d9f1aaa5cb08ae4338f314a06c
        - gcr.io/stackdriver-agents/stackdriver-logging-agent:0.5-1.5.36-1-k8s
      sizeBytes: 218848834
    - names:
        - k8s.gcr.io/fluentd-elasticsearch@sha256:b8c94527b489fb61d3d81ce5ad7f3ddbb7be71e9620a3a36e2bede2f2e487d73
        - k8s.gcr.io/fluentd-elasticsearch:v2.0.4
      sizeBytes: 135716379
    - names:
        - gcr.io/nathanmittler-istio-test/proxy_init@sha256:636193003388dbb67215ad58a8bae7f47a7ee7b378a0e80feb3301c82075f93e
        - gcr.io/nathanmittler-istio-test/proxy_init:latest
      sizeBytes: 118960405
    - names:
        - k8s.gcr.io/fluentd-gcp-scaler@sha256:bfd8ffbadf5cbfc7cd0944f5c13aaa8da421e3ab2322d610e64c4d7de9424c29
        - k8s.gcr.io/fluentd-gcp-scaler:0.3
      sizeBytes: 115128950
    - names:
        - gcr.io/google_containers/kube-proxy:v1.11.3-gke.18
        - k8s.gcr.io/kube-proxy:v1.11.3-gke.18
      sizeBytes: 102652198
    - names:
        - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0
        - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
      sizeBytes: 102319441
    - names:
        - k8s.gcr.io/prometheus-to-sd@sha256:6c0c742475363d537ff059136e5d5e4ab1f512ee0fd9b7ca42ea48bc309d1662
        - k8s.gcr.io/prometheus-to-sd:v0.3.1
      sizeBytes: 88077694
    - names:
        - k8s.gcr.io/kube-addon-manager@sha256:3519273916ba45cfc9b318448d4629819cb5fbccbb0822cce054dd8c1f68cb60
        - k8s.gcr.io/kube-addon-manager:v8.6
      sizeBytes: 78384272
    - names:
        - k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521
        - k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      sizeBytes: 76016169
    - names:
        - gcr.io/nathanmittler-istio-test/mixer@sha256:f7f39c2aa2445f0eeb974e052495bea0b182054b87fb130929e3e3a11b89178c
        - gcr.io/nathanmittler-istio-test/mixer:latest
      sizeBytes: 67962964
    - names:
        - k8s.gcr.io/ingress-gce-glbc-amd64@sha256:31d36bbd9c44caffa135fc78cf0737266fcf25e3cf0cd1c2fcbfbc4f7309cc52
        - k8s.gcr.io/ingress-gce-glbc-amd64:v1.1.1
      sizeBytes: 67801919
    - names:
        - gcr.io/nathanmittler-istio-test/mixer@sha256:8b95223097646bca9ea8de2d410a55aba287868e458a34cd6b414b1cd53a994d
      sizeBytes: 67690667
  nodeInfo:
    architecture: amd64
    bootID: a4687372-7eb1-4a05-913d-2f5cb2d97194
    containerRuntimeVersion: docker://17.3.2
    kernelVersion: 4.14.65+
    kubeProxyVersion: v1.11.3-gke.18
    kubeletVersion: v1.11.3-gke.18
    machineID: e11072430320008f166ee8b71cc69999
    operatingSystem: linux
    osImage: Container-Optimized OS from Google
    systemUUID: E1107243-0320-008F-166E-E8B71CC69999
---
apiVersion: v1
kind: Service
metadata:
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: KubeDNS
  name: kube-dns
spec:
  clusterIP: 10.43.240.11
  ports:
    - name: tcp-dns
      port: 53
      protocol: TCP
      targetPort: 53
  selector:
    k8s-app: kube-dns
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
---
apiVersion: v1
kind: Endpoints
metadata:
  creationTimestamp: 2018-02-12T15:48:44Z
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: KubeDNS
  name: kube-dns
subsets:
  - addresses:
      - ip: 10.40.1.5
        targetRef:
          kind: Pod
          name: kube-dns-548976df6c-d9kkv
      - ip: 10.40.1.4
        targetRef:
          kind: Pod
          name: kube-dns-548976df6c-d9kkv
    ports:
      - name: tcp-dns
        port: 53
        protocol: TCP
